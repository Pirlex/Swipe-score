{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f07d37",
   "metadata": {},
   "source": [
    "**0) Ancienne configuration du notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa40bec2-9b23-4916-abc3-2f0285bf4027",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T22:21:12.605218Z",
     "iopub.status.busy": "2025-11-01T22:21:12.604797Z",
     "iopub.status.idle": "2025-11-01T22:21:12.624420Z",
     "shell.execute_reply": "2025-11-01T22:21:12.623641Z",
     "shell.execute_reply.started": "2025-11-01T22:21:12.605187Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import requests  # ← Correction ici (pas resquests)\n",
    "import zipfile\n",
    "from IPython.display import Markdown, display, HTML  # ← display pas displa\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fb70b76-a916-4496-a27b-3a4d24c29c04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T22:21:12.625643Z",
     "iopub.status.busy": "2025-11-01T22:21:12.625205Z",
     "iopub.status.idle": "2025-11-01T22:21:12.805647Z",
     "shell.execute_reply": "2025-11-01T22:21:12.804950Z",
     "shell.execute_reply.started": "2025-11-01T22:21:12.625613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration AWS complétée\n"
     ]
    }
   ],
   "source": [
    "bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "bedrock = boto3.client('bedrock')\n",
    "s3 = boto3.client('s3')\n",
    "comprehend = boto3.client('comprehend')\n",
    "textract = boto3.client('textract')\n",
    "\n",
    "print(\"✅ Configuration AWS complétée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d902d80f",
   "metadata": {},
   "source": [
    "**1) Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123de7fb-a0ff-4fd2-81a2-daa6645003a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T15:24:19.011978Z",
     "iopub.status.busy": "2025-11-02T15:24:19.011453Z",
     "iopub.status.idle": "2025-11-02T15:24:20.929087Z",
     "shell.execute_reply": "2025-11-02T15:24:20.928091Z",
     "shell.execute_reply.started": "2025-11-02T15:24:19.011945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: defeatbeta-api in /opt/conda/lib/python3.11/site-packages (0.0.23)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.11/site-packages (1.39.11)\n",
      "Requirement already satisfied: sec-parser in /opt/conda/lib/python3.11/site-packages (0.58.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.3.3)\n",
      "Requirement already satisfied: duckdb>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from defeatbeta-api) (1.4.1)\n",
      "Requirement already satisfied: requests~=2.32.3 in /opt/conda/lib/python3.11/site-packages (from defeatbeta-api) (2.32.5)\n",
      "Requirement already satisfied: psutil>=7.0.0 in /opt/conda/lib/python3.11/site-packages (from defeatbeta-api) (7.1.3)\n",
      "Requirement already satisfied: pyfiglet>=1.0.2 in /opt/conda/lib/python3.11/site-packages (from defeatbeta-api) (1.0.4)\n",
      "Requirement already satisfied: urllib3~=2.5.0 in /opt/conda/lib/python3.11/site-packages (from defeatbeta-api) (2.5.0)\n",
      "Requirement already satisfied: tabulate>=0.9.0 in /opt/conda/lib/python3.11/site-packages (from defeatbeta-api) (0.9.0)\n",
      "Requirement already satisfied: numpy>=2.2.5 in /opt/conda/lib/python3.11/site-packages (from defeatbeta-api) (2.3.4)\n",
      "Requirement already satisfied: rich>=14.0.0 in /opt/conda/lib/python3.11/site-packages (from defeatbeta-api) (14.2.0)\n",
      "Requirement already satisfied: openai>=1.106.1 in /opt/conda/lib/python3.11/site-packages (from defeatbeta-api) (2.6.1)\n",
      "Requirement already satisfied: nltk>=3.9.2 in /opt/conda/lib/python3.11/site-packages (from defeatbeta-api) (3.9.2)\n",
      "Requirement already satisfied: botocore<1.40.0,>=1.39.11 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.39.11)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /opt/conda/lib/python3.11/site-packages (from boto3) (0.13.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (4.14.2)\n",
      "Requirement already satisfied: cssutils<3.0.0,>=2.11.1 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (2.11.1)\n",
      "Requirement already satisfied: frozendict<3.0.0,>=2.4.4 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (2.4.6)\n",
      "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (0.7.3)\n",
      "Requirement already satisfied: lxml<6.0.0,>=5.2.2 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (5.4.0)\n",
      "Requirement already satisfied: sec-downloader<0.12.0,>=0.11.1 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (0.11.2)\n",
      "Requirement already satisfied: xxhash<4.0.0,>=3.4.1 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->sec-parser) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->sec-parser) (4.15.0)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/lib/python3.11/site-packages (from cssutils<3.0.0,>=2.11.1->sec-parser) (10.8.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk>=3.9.2->defeatbeta-api) (8.3.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk>=3.9.2->defeatbeta-api) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk>=3.9.2->defeatbeta-api) (2025.10.23)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk>=3.9.2->defeatbeta-api) (4.67.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai>=1.106.1->defeatbeta-api) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai>=1.106.1->defeatbeta-api) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from openai>=1.106.1->defeatbeta-api) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from openai>=1.106.1->defeatbeta-api) (0.11.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from openai>=1.106.1->defeatbeta-api) (2.12.3)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai>=1.106.1->defeatbeta-api) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests~=2.32.3->defeatbeta-api) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests~=2.32.3->defeatbeta-api) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests~=2.32.3->defeatbeta-api) (2025.10.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=14.0.0->defeatbeta-api) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=14.0.0->defeatbeta-api) (2.19.2)\n",
      "Requirement already satisfied: sec-edgar-downloader in /opt/conda/lib/python3.11/site-packages (from sec-downloader<0.12.0,>=0.11.1->sec-parser) (5.0.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.106.1->defeatbeta-api) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.106.1->defeatbeta-api) (0.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=14.0.0->defeatbeta-api) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.106.1->defeatbeta-api) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.106.1->defeatbeta-api) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.106.1->defeatbeta-api) (0.4.2)\n",
      "Requirement already satisfied: pyrate-limiter>=3.6.0 in /opt/conda/lib/python3.11/site-packages (from sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser) (3.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install defeatbeta-api boto3 sec-parser pandas\n",
    "import sec_parser as sp\n",
    "import json\n",
    "import boto3\n",
    "import re\n",
    "import os\n",
    "import ast\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sec_parser as sp\n",
    "from sec_parser.processing_steps import (\n",
    "    TopSectionManagerFor10Q,\n",
    "    IndividualSemanticElementExtractor,\n",
    "    TopSectionTitleCheck,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5027fb69",
   "metadata": {},
   "source": [
    "**2) Helpers pour générer des versions markdown synthétiques des 10K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc25623-87de-4762-bddc-c74b5807b44a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T15:23:02.251504Z",
     "iopub.status.busy": "2025-11-02T15:23:02.251144Z",
     "iopub.status.idle": "2025-11-02T15:23:02.281024Z",
     "shell.execute_reply": "2025-11-02T15:23:02.280241Z",
     "shell.execute_reply.started": "2025-11-02T15:23:02.251477Z"
    }
   },
   "outputs": [],
   "source": [
    "# ---- Paramètres ----\n",
    "ITEMS_WANTED = {\"1\", \"3\", \"7\", \"8\"}  # Items à garder\n",
    "ITEM_ORDER = [\"1\", \"3\", \"7\", \"8\"]    # Ordre d'écriture\n",
    "\n",
    "# Détecte \"Item 1\", \"Item 1A\", \"ITEM 7 — ...\" (tolérant)\n",
    "ITEM_RE = re.compile(r\"item\\s+([0-9]+[a-z]?)\\b\", re.IGNORECASE)\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "\n",
    "def looks_like_toc(text: str) -> bool:\n",
    "    \"\"\"Heuristiques simples pour filtrer la table des matières.\"\"\"\n",
    "    t = (text or \"\").lower()\n",
    "    if \"table of contents\" in t:\n",
    "        return True\n",
    "    if re.search(r\"\\.{5,}\", text or \"\"):  # lignes avec \"........\"\n",
    "        return True\n",
    "    if len(text or \"\") < 30 and re.search(r\"\\s\\d{1,3}$\", text or \"\"):  # titre court + n° page\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def find_item_code(text: str) -> Optional[str]:\n",
    "    \"\"\"Retourne '1','3','7','8', etc. si la ligne ressemble à un début d'Item.\"\"\"\n",
    "    txt = normalize(text)\n",
    "    if looks_like_toc(txt):\n",
    "        return None\n",
    "    m = ITEM_RE.search(txt)\n",
    "    return m.group(1).upper() if m else None\n",
    "\n",
    "# ---------- Helpers robustes d'extraction de table en Markdown ----------\n",
    "def _coerce_to_html_string(x) -> Optional[str]:\n",
    "    \"\"\"Essaye de convertir un objet (HtmlTag, bs4.Tag, etc.) en str HTML.\"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            return x\n",
    "        if hasattr(x, \"get_source_code\"):\n",
    "            s = x.get_source_code()\n",
    "            if isinstance(s, str):\n",
    "                return s\n",
    "        return str(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def table_to_markdown_robust(node: sp.TableElement) -> str:\n",
    "    \"\"\"\n",
    "    1) Essaye node.table_to_markdown() si dispo\n",
    "    2) Sinon, récupère l'HTML du tableau et:\n",
    "       - tente TableParser(html).table_to_markdown() si dispo\n",
    "       - ou pandas.read_html + DataFrame.to_markdown() en dernier recours\n",
    "    \"\"\"\n",
    "    # 1) API directe\n",
    "    if hasattr(node, \"table_to_markdown\"):\n",
    "        try:\n",
    "            return node.table_to_markdown()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2) Récupérer l'HTML\n",
    "    html_table = None\n",
    "    if hasattr(node, \"get_source_code\"):\n",
    "        html_table = node.get_source_code()\n",
    "    if not isinstance(html_table, str) and hasattr(node, \"html_tag\"):\n",
    "        html_table = _coerce_to_html_string(node.html_tag)\n",
    "        if not isinstance(html_table, str) and hasattr(node.html_tag, \"get_source_code\"):\n",
    "            html_table = node.html_tag.get_source_code()\n",
    "    if not isinstance(html_table, str):\n",
    "        html_table = _coerce_to_html_string(html_table) or _coerce_to_html_string(node)\n",
    "\n",
    "    if isinstance(html_table, str) and \"<table\" in html_table.lower():\n",
    "        # a) TableParser -> DataFrame -> Markdown\n",
    "        try:\n",
    "            from sec_parser.semantic_elements.table_element.table_parser import TableParser\n",
    "            df = TableParser(html_table).parse_as_df()\n",
    "            if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "                return df.to_markdown(index=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # b) pandas.read_html -> Markdown\n",
    "        try:\n",
    "            dfs = pd.read_html(html_table)\n",
    "            for d in dfs:\n",
    "                if isinstance(d, pd.DataFrame) and not d.empty:\n",
    "                    return d.to_markdown(index=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 3) Dernier recours\n",
    "    return \"_Table not parsed (no available method for this sec_parser version)._\"\n",
    "\n",
    "# ---------- Pipeline principal : parse Items + insérer tables brutes ----------\n",
    "def parse_10k_html_with_tables(html_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse un 10-K via Edgar10QParser et retourne {code_item: markdown}\n",
    "    en injectant les tableaux aux bonnes positions (Markdown brut).\n",
    "    \"\"\"\n",
    "    html = Path(html_path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    elements = sp.Edgar10QParser().parse(html)  # contournement 10-K\n",
    "\n",
    "    # 1) Trouver tous les débuts d'Item\n",
    "    boundaries: List[tuple[int, str, str]] = []\n",
    "    for idx, el in enumerate(elements):\n",
    "        txt = getattr(el, \"text\", \"\") or \"\"\n",
    "        if len(txt) > 2000:\n",
    "            continue\n",
    "        code = find_item_code(txt)\n",
    "        if code:\n",
    "            boundaries.append((idx, code, normalize(txt)))\n",
    "    boundaries.sort(key=lambda t: t[0])\n",
    "\n",
    "    # 2) Construire le markdown par item\n",
    "    item_to_md: Dict[str, str] = {}\n",
    "    label_map = {\n",
    "        \"1\": \"Item 1 — Business\",\n",
    "        \"3\": \"Item 3 — Legal Proceedings\",\n",
    "        \"7\": \"Item 7 — Management’s Discussion and Analysis\",\n",
    "        \"8\": \"Item 8 — Financial Statements and Supplementary Data\",\n",
    "    }\n",
    "\n",
    "    for b_i, (start_idx, code, title_text) in enumerate(boundaries):\n",
    "        if code not in ITEMS_WANTED:\n",
    "            continue\n",
    "\n",
    "        # Fin de section = prochain Item\n",
    "        end_idx = len(elements)\n",
    "        for j in range(b_i + 1, len(boundaries)):\n",
    "            next_idx, _, _ = boundaries[j]\n",
    "            if next_idx > start_idx:\n",
    "                end_idx = next_idx\n",
    "                break\n",
    "\n",
    "        parts: List[str] = [f\"# {label_map.get(code, title_text)}\\n\"]\n",
    "\n",
    "        table_counter = 0\n",
    "        for k in range(start_idx + 1, end_idx):\n",
    "            node = elements[k]\n",
    "            if isinstance(node, sp.TableElement):\n",
    "                table_counter += 1\n",
    "                try:\n",
    "                    md_table = table_to_markdown_robust(node)\n",
    "                    parts.append(f\"\\n**Table {table_counter} (Item {code})**\\n\\n{md_table}\\n\")\n",
    "                except Exception as e:\n",
    "                    parts.append(f\"\\n**Table {table_counter} (Item {code}) — parsing error**\\n\\n_Error: {e}_\\n\")\n",
    "            else:\n",
    "                txt = (getattr(node, \"text\", \"\") or \"\").strip()\n",
    "                if txt and not looks_like_toc(txt):\n",
    "                    parts.append(txt + \"\\n\")\n",
    "\n",
    "        md = \"\\n\".join(parts).strip()\n",
    "        if code not in item_to_md or len(md) > len(item_to_md[code]):\n",
    "            item_to_md[code] = md\n",
    "\n",
    "    return item_to_md\n",
    "\n",
    "def save_items_markdown(item_to_md: Dict[str, str], out_md: str) -> None:\n",
    "    blocks: List[str] = []\n",
    "    for code in ITEM_ORDER:\n",
    "        if code in item_to_md:\n",
    "            blocks.append(item_to_md[code])\n",
    "    Path(out_md).write_text(\"\\n\\n---\\n\\n\".join(blocks) + \"\\n\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3e9eda",
   "metadata": {},
   "source": [
    "**3) Génération des versions markdown synthétiques des 10K dans le dossier fillings_markdown**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2d728a9-7b81-4ba3-8fcf-0d7e0355ca94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T15:23:12.230378Z",
     "iopub.status.busy": "2025-11-02T15:23:12.230104Z",
     "iopub.status.idle": "2025-11-02T15:23:12.239929Z",
     "shell.execute_reply": "2025-11-02T15:23:12.238994Z",
     "shell.execute_reply.started": "2025-11-02T15:23:12.230355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown enregistrés dans: /mnt/custom-file-systems/s3/shared/fillings_markdown\n",
      "Nombre de fichiers écrits: 2\n"
     ]
    }
   ],
   "source": [
    "def generate_10k_descriptions(wanted_tickers=[], max_iter=float('inf')):\n",
    "    \"\"\"\n",
    "    Génère des fichiers markdown simplifiés décrivant les rapports 10K \n",
    "    dans le dossier fillings_markdown\n",
    "    \"\"\"\n",
    "    root_in = Path(\"fillings\")\n",
    "    root_out = Path(\"fillings_markdown\")\n",
    "    root_out.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    num_files = 0\n",
    "    iter = 0\n",
    "    for folder in sorted(root_in.iterdir()):\n",
    "        iter += 1\n",
    "        if iter > max_iter:\n",
    "            break\n",
    "        if not folder.is_dir():\n",
    "            continue\n",
    "        if wanted_tickers and folder.name not in wanted_tickers:\n",
    "            continue\n",
    "    \n",
    "        out_dir = root_out\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "        for path in sorted(folder.glob(\"*.html\")):\n",
    "            try:\n",
    "                items_md = parse_10k_html_with_tables(str(path))\n",
    "                md_full = \"\\n\\n---\\n\\n\".join(\n",
    "                    items_md[k] for k in ITEM_ORDER if k in items_md\n",
    "                ).strip()\n",
    "    \n",
    "                if not md_full:\n",
    "                    md_full = \"_(no selected items found)_\"\n",
    "\n",
    "                out_path = out_dir / f\"{folder.name}.md\"\n",
    "                out_path.write_text(md_full + \"\\n\", encoding=\"utf-8\")\n",
    "                num_files += 1\n",
    "            except Exception as e:\n",
    "                # On écrit un fichier d'erreur à côté pour diagnostiquer\n",
    "                err_path = out_dir / f\"{path.stem}.error.md\"\n",
    "                err_path.write_text(f\"_Error parsing {path.name}: {e}_\\n\", encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"Markdown enregistrés dans: {root_out.resolve()}\")\n",
    "    print(f\"Nombre de fichiers écrits: {num_files}\")\n",
    "\n",
    "generate_10k_descriptions([\"AAPL\", \"ACGL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb35ff",
   "metadata": {},
   "source": [
    "**4) Mise en place des requêtes LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42b17e1c-6336-4202-9a67-2145ae51bc17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T15:26:31.592752Z",
     "iopub.status.busy": "2025-11-02T15:26:31.592478Z",
     "iopub.status.idle": "2025-11-02T15:26:31.634261Z",
     "shell.execute_reply": "2025-11-02T15:26:31.632940Z",
     "shell.execute_reply.started": "2025-11-02T15:26:31.592727Z"
    }
   },
   "outputs": [],
   "source": [
    "# AWS Bedrock Configuration\n",
    "AWS_REGION = os.environ.get(\"AWS_REGION\", \"us-east-1\")  # Change to your preferred region\n",
    "BEDROCK_MODEL_ID = \"anthropic.claude-3-5-sonnet-20241022-v2:0\"  # Claude 3.5 Sonnet\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=AWS_REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12b8630c-c5bf-43d1-813e-8b0167906c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_bedrock_model(prompt: str, max_tokens: int = 2000) -> str:\n",
    "    \"\"\"\n",
    "    Invoke Amazon Bedrock model with a prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the model\n",
    "        max_tokens: Maximum tokens in response\n",
    "    \n",
    "    Returns:\n",
    "        Model response text\n",
    "    \"\"\"\n",
    "    # Prepare the request body for Claude 3.5 Sonnet\n",
    "    body = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        # Invoke the model\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=BEDROCK_MODEL_ID,\n",
    "            body=body\n",
    "        )\n",
    "        \n",
    "        # Parse response\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return response_body['content'][0]['text']\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking Bedrock: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88972635",
   "metadata": {},
   "source": [
    "**5) Evaluation par LLM d'un 10K + helpers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09e024c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_json_block(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrait le premier bloc JSON-like { ... } d'une réponse LLM.\n",
    "    Tolère du texte avant/après. Renvoie la chaîne avec accolades incluses.\n",
    "    Lève ValueError si rien n'est trouvé.\n",
    "    \"\"\"\n",
    "    # Cherche le plus court bloc englobant équilibré à partir du 1er '{'\n",
    "    start = text.find(\"{\")\n",
    "    if start == -1:\n",
    "        raise ValueError(\"Réponse du LLM sans accolade '{' - JSON introuvable.\")\n",
    "    # Heuristique simple d'équilibrage des accolades\n",
    "    depth = 0\n",
    "    for i in range(start, len(text)):\n",
    "        if text[i] == \"{\":\n",
    "            depth += 1\n",
    "        elif text[i] == \"}\":\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                return text[start:i+1]\n",
    "    raise ValueError(\"Bloc JSON non équilibré - '}' manquante.\")\n",
    "\n",
    "def _parse_json_lenient(s: str) -> dict:\n",
    "    \"\"\"\n",
    "    Essaie plusieurs parseurs pour supporter:\n",
    "    - JSON standard (double quotes)\n",
    "    - dict Python (quotes simples)\n",
    "    - JSON encapsulé dans texte\n",
    "    \"\"\"\n",
    "    # 1) Essai direct JSON\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 2) Remplacement naïf des quotes simples -> doubles (sans toucher aux nombres)\n",
    "    try:\n",
    "        # D'abord, si c'est un dict Python valide, literal_eval est le plus sûr\n",
    "        return ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 3) Tentative: convertir les quotes simples de clés/valeurs en doubles via regex prudente\n",
    "    try:\n",
    "        s2 = re.sub(r\"(?P<q>')(?P<key>[^'\\\\]*?)\\1\\s*:\", r'\"\\g<key>\":', s)  # clés\n",
    "        s2 = re.sub(r\":\\s*(?P<q>')(?P<val>[^'\\\\]*?)\\1\", r': \"\\g<val>\"', s2) # valeurs textuelles\n",
    "        return json.loads(s2)\n",
    "    except Exception:\n",
    "        pass\n",
    "    raise ValueError(\"Impossible de parser la réponse du LLM en JSON/dict.\")\n",
    "\n",
    "def _validate_schema(payload: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Valide les clés, types et bornes. Convertit si besoin (ex: float -> int toléré si entier).\n",
    "    Lève ValueError si non conforme. Renvoie le dict (potentiellement casté).\n",
    "    \"\"\"\n",
    "    required_types = {\n",
    "        \"ton narcissique\": int,\n",
    "        \"répétitions\": int,\n",
    "        \"impact risque 1\": int,\n",
    "        \"probabilité risque 1\": float,\n",
    "        \"impact risque 2\": int,\n",
    "        \"probabilité risque 2\": float,\n",
    "        \"impact risque 3\": int,\n",
    "        \"probabilité risque 3\": float,\n",
    "        \"impact risque 4\": int,\n",
    "        \"probabilité risque 4\": float,\n",
    "        \"impact risque 5\": int,\n",
    "        \"probabilité risque 5\": float,\n",
    "        \"santé financière\": int,\n",
    "        \"perspective évolution\": int,\n",
    "        \"résumé\": str,\n",
    "    }\n",
    "\n",
    "    missing = [k for k in required_types if k not in payload]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Clés manquantes: {missing}\")\n",
    "\n",
    "    out = {}\n",
    "    # Bornes: notes sur 5 (0..5), proba en [0..1]\n",
    "    def as_int_0_5(x, key):\n",
    "        if isinstance(x, float) and x.is_integer():\n",
    "            x = int(x)\n",
    "        if not isinstance(x, int):\n",
    "            raise ValueError(f\"'{key}' doit être un entier (0..5).\")\n",
    "        if not (0 <= x <= 5):\n",
    "            raise ValueError(f\"'{key}' hors bornes (0..5): {x}\")\n",
    "        return x\n",
    "\n",
    "    def as_float_0_1(x, key):\n",
    "        # Autoriser string \"0.7\" -> float\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                x = float(x.strip().replace(\",\", \".\"))\n",
    "            except Exception:\n",
    "                raise ValueError(f\"'{key}' doit être un float (0..1).\")\n",
    "        if not isinstance(x, (int, float)):\n",
    "            raise ValueError(f\"'{key}' doit être un float (0..1).\")\n",
    "        x = float(x)\n",
    "        if not (0.0 <= x <= 1.0):\n",
    "            raise ValueError(f\"'{key}' hors bornes (0..1): {x}\")\n",
    "        return x\n",
    "\n",
    "    for key, typ in required_types.items():\n",
    "        val = payload[key]\n",
    "        if key == \"résumé\":\n",
    "            if not isinstance(val, str) or not val.strip():\n",
    "                raise ValueError(\"'résumé' doit être une chaîne non vide.\")\n",
    "            out[key] = val.strip()\n",
    "        elif \"probabilité\" in key:\n",
    "            out[key] = as_float_0_1(val, key)\n",
    "        else:\n",
    "            out[key] = as_int_0_5(val, key)\n",
    "\n",
    "    return out\n",
    "\n",
    "def assess_1_10K(path: str) -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Analyse le fichier markdown d'un 10-K, interroge un LLM et renvoie un JSON validé. Lève une\n",
    "    erreur en cas de problème.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Chemin vers le fichier markdown du 10-K.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, object]: JSON d'évaluation (conforme au schéma spécifié).\n",
    "    \"\"\"\n",
    "    path_md = Path(path)\n",
    "    md_content = path_md.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    prompt = (\n",
    "        \"Tu analyses et évalues des rapports financiers annuels 10-K d'entreprises cotées \"\n",
    "        \"en bourse aux États-Unis. Ces documents sont rédigés par les entreprises elles-mêmes et ne sont \"\n",
    "        \"pas toujours complètement honnêtes. ILs peuvent essayer de cacher la vérité. Tu dois donc \"\n",
    "        \"être méfiant vis-à-vis de la véracité des propos. Le ton de l'écriture, les répétitions peuvent \"\n",
    "        \"donc être utiles pour mieux cerner l'entreprise. Tu vas devoir faire plusieurs analyses mentalement \"\n",
    "        \" et uniquement envoyer un JSON de réponse. Tu dois évaluer en imaginant que tu compares le document \"\n",
    "        \"fourni avec celui d'autres entreprises.\"\n",
    "        \"- Analyse le ton narcissique du texte puis donne une note sur 5 où 0 est très narcissique. \"\n",
    "        \"- Evalue le taux de répétitions et de mentions inutiles dans le texte puis donne une note sur 5. \"\n",
    "        \" La note de 5 signifie qu'il n'y a presque pas de répétitions.\"\n",
    "        \"- Cite 5 risques pour l'entreprise. Pour chacun, précise l'impact que ça peut avoir sur le \"\n",
    "        \"titre boursier de l'entreprise ainsi que la probabilité que le risque ait cet impact. Evalue \"\n",
    "        \"l'impact sur 5 où 0 signifie qu'il y a aucun impact et donne la probabilité.\"\n",
    "        \"- Evalue la santé financière sur 5. 5 correspond à une très bonne santé.\"\n",
    "        \"- Donne une note sur 5 à la perspective d'évolution de l'action boursière. 5 correspond à la \"\n",
    "        \"meilleure perspective possible. \"\n",
    "        \"Tu dois uniquement renvoyer une réponse sous le format JSON suivant : \"\n",
    "        \"{'ton narcissique': int,\"\n",
    "        \"'répétitions': int,\"\n",
    "        \"'impact risque 1': int,\"\n",
    "        \"'probabilité risque 1': float,\"\n",
    "        \"'impact risque 2': int,\"\n",
    "        \"'probabilité risque 2': float,\"\n",
    "        \"'impact risque 3': int,\"\n",
    "        \"'probabilité risque 3': float,\"\n",
    "        \"'impact risque 4': int,\"\n",
    "        \"'probabilité risque 4': float,\"\n",
    "        \"'impact risque 5': int,\"\n",
    "        \"'probabilité risque 5': float,\"\n",
    "        \"'santé financière': int,\"\n",
    "        \"'perspective évolution': int,\"\n",
    "        \"'résumé': str\"\n",
    "        \"}\"\n",
    "        \"La clé 'résumé' correspond à un petit texte en français qui explique ce qui a été analysé \"\n",
    "        \"sans parler des notes. Il faut expliquer en quoi il s'agit d'une action fiable ou non en \"\n",
    "        \"justifiant tes arguments avec ce que tu as vu. Voici le rapport 10K :\\n\"\n",
    "    )\n",
    "\n",
    "    # Appel LLM (fonction utilitaire fournie ailleurs dans ton projet)\n",
    "    response_text = invoke_bedrock_model(prompt + md_content, max_tokens=200000)\n",
    "\n",
    "    # Extraction + parsing\n",
    "    json_block = _extract_json_block(response_text)\n",
    "    payload = _parse_json_lenient(json_block)\n",
    "\n",
    "    # Validation stricte\n",
    "    validated = _validate_schema(payload)\n",
    "\n",
    "    return validated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5356cd",
   "metadata": {},
   "source": [
    "**6) Evaluation de tous les 10K simplifiés présents dans fillings_markdown**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2501134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ton narcissique': 4,\n",
       "  'répétitions': 4,\n",
       "  'impact risque 1': 4,\n",
       "  'probabilité risque 1': 0.7,\n",
       "  'impact risque 2': 4,\n",
       "  'probabilité risque 2': 0.6,\n",
       "  'impact risque 3': 3,\n",
       "  'probabilité risque 3': 0.5,\n",
       "  'impact risque 4': 3,\n",
       "  'probabilité risque 4': 0.4,\n",
       "  'impact risque 5': 3,\n",
       "  'probabilité risque 5': 0.4,\n",
       "  'santé financière': 4,\n",
       "  'perspective évolution': 4,\n",
       "  'résumé': \"Agilent Technologies est un leader mondial dans les sciences de la vie, le diagnostic et les marchés de la chimie appliquée. L'entreprise montre une bonne santé financière avec une trésorerie solide ($1.3B), une bonne diversification géographique et une croissance stable. Les principaux risques incluent: dépendance aux dépenses d'investissement des clients, exposition aux fluctuations économiques mondiales, risques liés à la chaîne d'approvisionnement, concurrence accrue et risques réglementaires. Malgré une baisse des revenus de 5% en 2024, l'entreprise maintient de bonnes marges et continue d'investir dans l'innovation. La stratégie d'acquisition ciblée (ex: BIOVECTRA) et la solidité du segment CrossLab démontrent un potentiel de croissance à long terme.\",\n",
       "  'action': 'A'},\n",
       " {'ton narcissique': 2,\n",
       "  'répétitions': 4,\n",
       "  'impact risque 1': 4,\n",
       "  'probabilité risque 1': 0.7,\n",
       "  'impact risque 2': 4,\n",
       "  'probabilité risque 2': 0.6,\n",
       "  'impact risque 3': 3,\n",
       "  'probabilité risque 3': 0.5,\n",
       "  'impact risque 4': 3,\n",
       "  'probabilité risque 4': 0.4,\n",
       "  'impact risque 5': 3,\n",
       "  'probabilité risque 5': 0.3,\n",
       "  'santé financière': 5,\n",
       "  'perspective évolution': 4,\n",
       "  'résumé': \"Apple présente une excellente santé financière avec des revenus diversifiés géographiquement et par produits. Les principaux risques identifiés sont : la dépendance aux fournisseurs asiatiques, les tensions géopolitiques notamment avec la Chine, la forte concurrence sur les marchés matures, les enjeux de propriété intellectuelle et les contraintes réglementaires croissantes. Malgré ces risques, l'entreprise maintient une position dominante sur ses marchés avec une forte capacité d'innovation et de solides marges. La trésorerie importante et la génération régulière de cash-flow offrent une bonne visibilité. Les perspectives restent favorables grâce à la diversification vers les services et le développement de nouveaux produits comme le Vision Pro.\",\n",
       "  'action': 'AAPL'},\n",
       " {'ton narcissique': 3,\n",
       "  'répétitions': 4,\n",
       "  'impact risque 1': 4,\n",
       "  'probabilité risque 1': 0.7,\n",
       "  'impact risque 2': 4,\n",
       "  'probabilité risque 2': 0.6,\n",
       "  'impact risque 3': 3,\n",
       "  'probabilité risque 3': 0.5,\n",
       "  'impact risque 4': 3,\n",
       "  'probabilité risque 4': 0.4,\n",
       "  'impact risque 5': 3,\n",
       "  'probabilité risque 5': 0.4,\n",
       "  'santé financière': 4,\n",
       "  'perspective évolution': 4,\n",
       "  'résumé': \"Arch Capital présente un profil solide avec une diversification géographique et une présence dans plusieurs segments d'assurance (réassurance, assurance directe, assurance hypothécaire). L'entreprise montre une bonne santé financière avec des fonds propres importants et une gestion prudente des risques. Les principaux risques identifiés sont : 1) L'exposition aux catastrophes naturelles et au changement climatique 2) Les changements réglementaires notamment liés à Pillar II 3) La dépendance aux courtiers et partenaires stratégiques 4) Les risques cybersécurité 5) Les fluctuations des marchés financiers impactant les investissements. L'entreprise démontre une bonne capacité d'adaptation et d'innovation, notamment dans l'utilisation de l'IA, tout en maintenant une approche disciplinée de la souscription. La croissance récente des effectifs et les acquisitions stratégiques témoignent d'une dynamique positive.\",\n",
       "  'action': 'ACGL'},\n",
       " {'ton narcissique': 2,\n",
       "  'répétitions': 4,\n",
       "  'impact risque 1': 4,\n",
       "  'probabilité risque 1': 0.7,\n",
       "  'impact risque 2': 4,\n",
       "  'probabilité risque 2': 0.6,\n",
       "  'impact risque 3': 3,\n",
       "  'probabilité risque 3': 0.5,\n",
       "  'impact risque 4': 3,\n",
       "  'probabilité risque 4': 0.4,\n",
       "  'impact risque 5': 3,\n",
       "  'probabilité risque 5': 0.3,\n",
       "  'santé financière': 5,\n",
       "  'perspective évolution': 4,\n",
       "  'résumé': \"Microsoft présente une excellente santé financière avec une forte croissance des revenus et de la rentabilité. L'entreprise est bien positionnée sur des marchés porteurs (cloud, IA, gaming) et diversifie ses sources de revenus. Les principaux risques identifiés sont : 1) Problèmes fiscaux avec l'IRS qui réclame $28.9B, 2) Dépendance aux composants critiques (GPU) pour les datacenters, 3) Concurrence accrue sur l'IA, 4) Risques de cybersécurité, 5) Incertitudes réglementaires (GDPR, antitrust). Néanmoins, la solidité du bilan, la diversification des activités et le leadership technologique rendent l'action attractive malgré ces risques. L'acquisition d'Activision Blizzard renforce encore la position dans le gaming.\",\n",
       "  'action': 'MSFT'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assess_all_10K(max_iter=float('inf')) -> List[object]:\n",
    "    \"\"\"\n",
    "    Analyse tous les fichiers markdown 10-K dans 'fillings_markdown' et renvoie\n",
    "    une liste des json d'évaluation. \n",
    "    Args:\n",
    "        max_iter (int): Nombre maximum de fichiers à traiter (par défaut tous).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    iter = 0\n",
    "    for md_file in Path(\"fillings_markdown\").glob(\"*.md\"):\n",
    "        iter += 1\n",
    "        if iter > max_iter:\n",
    "            break\n",
    "        try:\n",
    "            result = assess_1_10K(str(md_file))\n",
    "            result[\"action\"] = md_file.stem\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            results.append({\"action\": md_file.stem, \"error\": str(e)})\n",
    "    return results\n",
    "\n",
    "#a = assess_all_10K()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e568048b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.019, 3.142, 2.942, 3.1189999999999998]\n"
     ]
    }
   ],
   "source": [
    "def compute_scores(list_jsons: list) -> list[float]:\n",
    "    \"\"\"\n",
    "    Donne le score final à partir d'un json avec toutes les informations mais pas encore formatté\n",
    "    \"\"\"\n",
    "    w_ton = 0.05\n",
    "    w_risk = 0.45\n",
    "    w_repetitions = 0.1\n",
    "    w_sante_financiere = 0.2\n",
    "    w_perspective = 0.2\n",
    "\n",
    "    result = []\n",
    "    for json in list_jsons:\n",
    "        score_risk = 0\n",
    "        for risk in range(1, 6):\n",
    "            score_risk += float(json[f'impact risque {risk}'])*float(json[f'probabilité risque {risk}'])\n",
    "        score = w_ton*float(json['ton narcissique']) + w_repetitions*float(json['répétitions']) + w_sante_financiere*float(json['santé financière']) + w_perspective*float(json['perspective évolution'])+ w_risk*score_risk/5\n",
    "        result.append(score)\n",
    "    return result\n",
    "\n",
    "#print(compute_scores(a))\n",
    "\n",
    "def group_jsons(list_10K: list, list_yahoo: list) -> list:\n",
    "    \"\"\"\n",
    "    Regroupe les données des évaluations 10-K avec les données Yahoo Finance.\n",
    "    \n",
    "    Args:\n",
    "        list_10K (list): Liste des évaluations 10-K.\n",
    "        list_yahoo (list): Liste des données Yahoo Finance.\n",
    "\n",
    "    Returns:\n",
    "        list: Liste des dictionnaires regroupés par action.\n",
    "    \"\"\"\n",
    "    grouped = []\n",
    "    yahoo_dict = {item[\"action\"]: item for item in list_yahoo}\n",
    "    seen_actions = set()\n",
    "    for item_10K in list_10K:\n",
    "        action = item_10K.get(\"action\")\n",
    "        seen_actions.add(action)\n",
    "        yahoo_data = yahoo_dict.get(action, {})\n",
    "        combined = {**item_10K, **yahoo_data}\n",
    "        grouped.append(combined)\n",
    "\n",
    "    for action, yahoo_data in yahoo_dict.items():\n",
    "        if action not in seen_actions:\n",
    "            grouped.append(yahoo_data)\n",
    "\n",
    "    return grouped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a0736f",
   "metadata": {},
   "source": [
    "**Méthode alternative pour les logos des entreprises : crée le dossier sp500_logos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966c98c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '²' (U+00B2) (3935029318.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    HEADERS = {\"User-Agent\": \"sp500-logos/1.0\"}²\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '²' (U+00B2)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import requests, pandas as pd, time, pathlib, urllib.parse\n",
    "\n",
    "WIKI_LIST_URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "LOGO_TOKEN = \"VOTRE_TOKEN_LOGO_DEV\"           # https://www.logo.dev/ (inscription gratuite)\n",
    "OUT_DIR = pathlib.Path(\"sp500_logos\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "HEADERS = {\"User-Agent\": \"sp500-logos/1.0\"}\n",
    "\n",
    "# 1) Constituants S&P500 depuis Wikipédia (éviter 403)\n",
    "resp = requests.get(WIKI_LIST_URL, headers=HEADERS, timeout=30)\n",
    "resp.raise_for_status()  # lève une erreur si 4xx/5xx\n",
    "# Passer le HTML brut à pandas (plus fiable que lui laisser faire la requête)\n",
    "tables = pd.read_html(resp.text, flavor=\"lxml\")  # ou bs4 si tu préfères\n",
    "# Trouver la table qui contient bien 'Symbol' et 'Security'\n",
    "df = next(t for t in tables if {\"Symbol\",\"Security\"}.issubset(set(t.columns)))\n",
    "df = df[[\"Symbol\", \"Security\"]].rename(columns={\"Security\": \"Company\"})\n",
    "\n",
    "# 2) Domaine officiel via Wikidata (P856)\n",
    "def wikidata_official_site(qid):\n",
    "    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n",
    "    j = requests.get(url, headers=HEADERS, timeout=20).json()\n",
    "    claims = j[\"entities\"][qid][\"claims\"]\n",
    "    if \"P856\" in claims:\n",
    "        return claims[\"P856\"][0][\"mainsnak\"][\"datavalue\"][\"value\"]\n",
    "    return None\n",
    "\n",
    "def wikipedia_title_to_qid(title):\n",
    "    api = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\"action\":\"query\",\"prop\":\"pageprops\",\"titles\":title,\"format\":\"json\"}\n",
    "    r = requests.get(api, params=params, headers=HEADERS, timeout=20).json()\n",
    "    page = next(iter(r[\"query\"][\"pages\"].values()))\n",
    "    return page.get(\"pageprops\", {}).get(\"wikibase_item\")\n",
    "\n",
    "def guess_domain(url):\n",
    "    if not url: return None\n",
    "    u = urllib.parse.urlparse(url if url.startswith(\"http\") else \"https://\" + url)\n",
    "    return u.netloc or u.path\n",
    "\n",
    "domains = []\n",
    "for title in df[\"Company\"].tolist():\n",
    "    qid = wikipedia_title_to_qid(title)\n",
    "    site = wikidata_official_site(qid) if qid else None\n",
    "    domains.append(guess_domain(site))\n",
    "    time.sleep(0.1)  # être gentil avec les APIs\n",
    "\n",
    "df[\"domain\"] = domains\n",
    "\n",
    "# 3) Logos via Logo.dev (image simple)\n",
    "def logo_url(domain, size=256, fmt=\"png\"):\n",
    "    return f\"https://img.logo.dev/{domain}?token={LOGO_TOKEN}&size={size}&format={fmt}\"\n",
    "\n",
    "saved = []\n",
    "for sym, comp, dom in df[[\"Symbol\",\"Company\",\"domain\"]].itertuples(index=False):\n",
    "    if not dom: \n",
    "        saved.append(None); \n",
    "        continue\n",
    "    url = logo_url(dom)\n",
    "    try:\n",
    "        img = requests.get(url, headers=HEADERS, timeout=20)\n",
    "        if img.ok and img.headers.get(\"Content-Type\",\"\").startswith((\"image/\",)):\n",
    "            ext = img.headers[\"Content-Type\"].split(\"/\")[-1].split(\";\")[0]\n",
    "            path = OUT_DIR / f\"{sym}_{dom}.{ext}\"\n",
    "            path.write_bytes(img.content)\n",
    "            saved.append(str(path))\n",
    "        else:\n",
    "            saved.append(None)\n",
    "    except Exception:\n",
    "        saved.append(None)\n",
    "    time.sleep(0.05)\n",
    "\n",
    "df[\"logo_file\"] = saved\n",
    "df.to_csv(\"sp500_logos_index.csv\", index=False)\n",
    "print(\"OK → fichiers dans\", OUT_DIR.resolve())\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
